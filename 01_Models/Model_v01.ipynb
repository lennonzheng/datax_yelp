{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ca29be-3e64-4548-a224-bf91e2655c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make compatible with Python 2 and Python 3\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3604d5a-cd7c-4e43-aecd-a742ec6e9535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lennonzheng/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lennonzheng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lennonzheng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lennonzheng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# regular expressions, text parsing, and ML classifiers\n",
    "import re\n",
    "import nltk\n",
    "import bs4 as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "\n",
    "# download NLTK classifiers\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# import ml classifiers\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "from nltk.stem import PorterStemmer     # parsing/stemmer\n",
    "from nltk.tag import pos_tag            # parts-of-speech tagging\n",
    "from nltk.corpus import wordnet         # sentiment scores\n",
    "from nltk.stem import WordNetLemmatizer # stem and context\n",
    "from nltk.corpus import stopwords       # stopwords\n",
    "from nltk.util import ngrams            # ngram iterator\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a35a63-0df6-4e96-9c1a-79311a45b97d",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c032a7f5-604d-442a-a356-d368ae65b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data = pd.read_csv(\"/Users/lennonzheng/Downloads/DataX/Project/yelp_dataset/austin_only/austin_review.csv\")\n",
    "austin_rest = pd.read_csv(\"/Users/lennonzheng/Downloads/DataX/Project/yelp_dataset/austin_only/austin_rest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57e0c1b9-1bcb-4897-aa6e-3fdb0f441b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## takes in a resturant name (user inputs), or cusine type\n",
    "## match the Business id from resturant df to review df, get all reviews, and return a filtered review df\n",
    "\n",
    "def filter_reviews(rest_name, cusine=None, rest_df=austin_rest, rev_df=reviews_data):\n",
    "    rest_id = austin_rest.loc[austin_rest.name == rest_name].business_id.values[0]\n",
    "    train = rev_df.loc[reviews_data.business_id == rest_id]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2549547d-420f-482c-8528-8c924d0bf29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = filter_reviews(\"Franklin Barbecue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e440f-584b-4281-bbf2-b7bb202eadba",
   "metadata": {},
   "source": [
    "## Preparing data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4fdd7ed4-8a8c-4fe4-b915-a71390fb48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_cleaner(review, lemmatize=True, stem=False):\n",
    "    '''\n",
    "        Clean and preprocess a review.\n",
    "            1. Remove HTML tags\n",
    "            2. Extract emoticons\n",
    "            3. Use regex to remove all special characters (only keep letters)\n",
    "            4. Make strings to lower case and tokenize / word split reviews\n",
    "            5. Remove English stopwords\n",
    "            6. Lemmatize\n",
    "            7. Rejoin to one string\n",
    "        \n",
    "        @review (type:str) is an unprocessed review string\n",
    "        @return (type:str) is a 6-step preprocessed review string\n",
    "    '''\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    cleaned_reviews=[]\n",
    "    for i,review in enumerate(train['text']):\n",
    "        # batching step notification\n",
    "        if( (i+1)%1000 == 0 ):\n",
    "            print(\"Done with %d reviews\" %(i+1))\n",
    "        \n",
    "        \n",
    "        #1. Remove HTML tags\n",
    "        review = bs.BeautifulSoup(review).text    \n",
    "\n",
    "        #2. Use regex to find emoticons\n",
    "        emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', review)\n",
    "\n",
    "        #3. Remove punctuation\n",
    "        review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "\n",
    "        #4. Tokenize into words (all lower case)\n",
    "        review = review.lower().split()\n",
    "\n",
    "        #5. Remove stopwords\n",
    "        eng_stopwords = set(stopwords.words(\"english\"))\n",
    "        \n",
    "        #6. Lemmatize \n",
    "        clean_review=[]\n",
    "        for word in review:\n",
    "            if word not in eng_stopwords:\n",
    "                if lemmatize is True:\n",
    "                    word=wnl.lemmatize(word)\n",
    "                elif stem is True:\n",
    "                    if word == 'oed':\n",
    "                        continue\n",
    "                    word=ps.stem(word)\n",
    "                clean_review.append(word)\n",
    "\n",
    "        #7. Join the review to one sentence\n",
    "        review_processed = ' '.join(clean_review+emoticons)\n",
    "        cleaned_reviews.append(review_processed)\n",
    "    \n",
    "\n",
    "    return(cleaned_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470606ca-ece9-4328-9bb3-ba63ba16310d",
   "metadata": {},
   "source": [
    "## Train and validate sentiment analysis model using Random Forest Classifier (RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a8196a7-2452-4926-a8c3-09fae6befa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics                          # evaluating model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#CountVectorizer can actucally handle a lot of the preprocessing for us\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3abd8636-ac62-4282-a2af-2300121e05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_sentiment(cleaned_reviews, y=train[\"stars\"], ngram=1, max_features=1000):\n",
    "    '''\n",
    "        This function will:\n",
    "            1. split data into train and test set.\n",
    "            2. get n-gram counts from cleaned reviews \n",
    "            3. train a random forest model using train n-gram counts and y (labels)\n",
    "            4. test the model on your test split\n",
    "            5. print accuracy of sentiment prediction on test and training data\n",
    "            6. print confusion matrix on test data results\n",
    "\n",
    "            To change n-gram type, set value of ngram argument\n",
    "            To change the number of features you want the countvectorizer to generate, set the value of max_features argument\n",
    "            \n",
    "            @cleaned_review (type:str) is preprocessed string from review_cleaner()\n",
    "            @return none\n",
    "    '''\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool, here we show more keywords \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, ngram),\n",
    "                                 analyzer = \"word\",   \n",
    "                                 tokenizer = None,    \n",
    "                                 preprocessor = None, \n",
    "                                 stop_words = None,   \n",
    "                                 max_features = max_features) \n",
    "    \n",
    "    # train / test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(cleaned_reviews, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 50 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "    # predict\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    # validation\n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    print(\" The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    print()\n",
    "    print('CONFUSION MATRIX:')\n",
    "    print('         Predicted')\n",
    "    print('          neg pos')\n",
    "    print(' Actual')\n",
    "    c=confusion_matrix(y_test, test_predictions)\n",
    "    print('     neg  ',c[0])\n",
    "    print('     pos  ',c[1])\n",
    "\n",
    "    #Extract feature importance\n",
    "    print('\\nTOP TEN IMPORTANT FEATURES:')\n",
    "    importances = forest.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_10 = indices[:20]\n",
    "    print([vectorizer.get_feature_names()[ind] for ind in top_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef3d3e-5637-4fbf-8327-dfbb3aacf5fe",
   "metadata": {},
   "source": [
    "## Train and test  Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80eab1-be7c-4c3c-8379-9bf587c2123d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3711a158-18ee-4947-ba03-abe87178f7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1000 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 5000 reviews\n"
     ]
    }
   ],
   "source": [
    "# Clean the reviews in the training set 'train' using review_cleaner function defined above\n",
    "# Here we use the original reviews without lemmatizing and stemming\n",
    "original_clean_reviews_lemmatize = review_cleaner(train['text'], lemmatize=True, stem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e462ce-e76c-4e84-b3e9-54e316310ff1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Train RFC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "253e9770-9c8a-4439-9a6c-5f48df629c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.9995069033530573 \n",
      " The validation accuracy is:  0.7438423645320197\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [ 1  0  0  0 23]\n",
      "     pos   [ 0  0  1  1 27]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['brisket', 'good', 'better', 'line', 'best', 'wait', 'bbq', 'ever', 'hour', 'star', 'franklin', 'food', 'rib', 'long', 'place', 'meat', 'would', 'get', 'time', 'pretty good']\n"
     ]
    }
   ],
   "source": [
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews_lemmatize, y=train[\"stars\"], ngram=2, max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4febb04-0f4b-4492-9c5c-f8eb54e55be6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
